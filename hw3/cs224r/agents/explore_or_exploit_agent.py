from collections import OrderedDict

from cs224r.critics.dqn_critic import DQNCritic
from cs224r.critics.cql_critic import CQLCritic
from cs224r.infrastructure.replay_buffer import ReplayBuffer
from cs224r.infrastructure.utils import *
from cs224r.infrastructure import pytorch_util as ptu
from cs224r.policies.argmax_policy import ArgMaxPolicy
from cs224r.infrastructure.dqn_utils import MemoryOptimizedReplayBuffer
from cs224r.exploration.rnd_model import RNDModel
from .dqn_agent import DQNAgent
import numpy as np


class ExplorationOrExploitationAgent(DQNAgent):
    def __init__(self, env, agent_params, normalize_rnd=True, rnd_gamma=0.99):
        super(ExplorationOrExploitationAgent, self).__init__(env, agent_params)
        
        self.replay_buffer = MemoryOptimizedReplayBuffer(100000, 1, float_obs=True)
        self.num_exploration_steps = agent_params['num_exploration_steps']
        self.offline_exploitation = agent_params['offline_exploitation']

        self.exploitation_critic = CQLCritic(agent_params, self.optimizer_spec)
        self.exploration_critic = DQNCritic(agent_params, self.optimizer_spec)
        
        self.exploration_model = RNDModel(agent_params, self.optimizer_spec)
        self.explore_weight_schedule = agent_params['explore_weight_schedule']
        self.exploit_weight_schedule = agent_params['exploit_weight_schedule']
        
        self.use_boltzmann = agent_params['use_boltzmann']
        if self.use_boltzmann:
            self.actor = ArgMaxPolicy(self.exploitation_critic, use_boltzmann=True)
            self.learning_starts = 0
        else:
            self.actor = ArgMaxPolicy(self.exploration_critic)

        self.eval_policy = ArgMaxPolicy(self.exploitation_critic)
        self.exploit_rew_shift = agent_params['exploit_rew_shift']
        self.exploit_rew_scale = agent_params['exploit_rew_scale']
        self.eps = agent_params['eps']

        self.running_rnd_rew_std = 1
        self.normalize_rnd = normalize_rnd
        self.rnd_gamma = rnd_gamma

    def train(self, ob_no, ac_na, re_n, next_ob_no, terminal_n):
        log = {}

        if self.t > self.num_exploration_steps:
            # After exploration is over, set the actor to optimize the extrinsic critic
            self.actor.set_critic(self.exploitation_critic)
            self.actor.use_boltzmann = False

        # CQL 
        if (self.t > self.learning_starts
                and self.t % self.learning_freq == 0
                and self.replay_buffer.can_sample(self.batch_size)
        ):

          
            explore_weight = self.explore_weight_schedule.value(self.t)
            exploit_weight = self.exploit_weight_schedule.value(self.t)

            expl_bonus = self.exploration_model.forward_np(ob_no)
            if self.normalize_rnd:
                expl_bonus = normalize(expl_bonus, 0, self.running_rnd_rew_std)
                self.running_rnd_rew_std = (self.rnd_gamma * self.running_rnd_rew_std 
                    + (1 - self.rnd_gamma) * expl_bonus.std())


            mixed_reward = explore_weight * expl_bonus + exploit_weight * re_n


            env_reward = (re_n + self.exploit_rew_shift) * self.exploit_rew_scale

            # Update Critics And Exploration Model #
            expl_model_loss = self.exploration_model.update(next_ob_no)
            exploration_critic_loss = self.exploration_critic.update(ob_no, ac_na, next_ob_no, mixed_reward, terminal_n)
            exploitation_critic_loss = self.exploitation_critic.update(ob_no, ac_na, next_ob_no, env_reward, terminal_n)

            # Target Networks #
            if self.num_param_updates % self.target_update_freq == 0:
                self.exploitation_critic.update_target_network()
                self.exploration_critic.update_target_network()

            # Logging #
            log['Exploration Critic Loss'] = exploration_critic_loss['Training Loss']
            log['Exploitation Critic Loss'] = exploitation_critic_loss['Training Loss']
            log['Exploration Model Loss'] = expl_model_loss

            # <DONE>: Uncomment these lines after completing cql_critic.py
            log['Exploitation Data q-values'] = exploitation_critic_loss['Data q-values']
            log['Exploitation OOD q-values'] = exploitation_critic_loss['OOD q-values']
            log['Exploitation CQL Loss'] = exploitation_critic_loss['CQL Loss']

            self.num_param_updates += 1

        self.t += 1
        return log


    def step_env(self):
        """
            Step the env and store the transition
            At the end of this block of code, the simulator should have been
            advanced one step, and the replay buffer should contain one more transition.
            Note that self.last_obs must always point to the new latest observation.
        """
        if (not self.offline_exploitation) or (self.t <= self.num_exploration_steps):
            self.replay_buffer_idx = self.replay_buffer.store_frame(self.last_obs)

        #perform_random_action = np.random.random() < self.eps or self.t < self.learning_starts

        # if perform_random_action:
        #     # if np.random.random() < 0.5:
        #     #     action = self.env.get_optimal_action(self.env.state)
        #     # else:
        #     #     action = self.env.action_space.sample()
        #     action = self.env.get_optimal_action(self.env.state)
        # else:
        #     processed = self.replay_buffer.encode_recent_observation()
        #     action = self.actor.get_action(processed)
        action = self.env.get_optimal_action(self.env.state)

        next_obs, reward, done, info = self.env.step(action)
        self.last_obs = next_obs.copy()

        if (not self.offline_exploitation) or (self.t <= self.num_exploration_steps):
            self.replay_buffer.store_effect(self.replay_buffer_idx, action, reward, done)

        if done:
            self.last_obs = self.env.reset()
